{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\fudal\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForMaskedLM: ['cls.sso.sso_relationship.weight', 'cls.sso.sso_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-uncased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at henryk/bert-base-multilingual-cased-finetuned-polish-squad1 were not used when initializing BertForMaskedLM: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at henryk/bert-base-multilingual-cased-finetuned-polish-squad1 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_AH = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "model_AH = AutoModelWithLMHead.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "\n",
    "tokenizer_DK = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
    "model_DK = AutoModelWithLMHead.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
    "\n",
    "tokenizer_BM = AutoTokenizer.from_pretrained(\"henryk/bert-base-multilingual-cased-finetuned-polish-squad1\")\n",
    "model_BM = AutoModelWithLMHead.from_pretrained(\"henryk/bert-base-multilingual-cased-finetuned-polish-squad1\")\n",
    "\n",
    "M_T = [ \n",
    "[model_AH,tokenizer_AH],\n",
    "[model_DK,tokenizer_DK],\n",
    "[model_BM,tokenizer_BM],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_T_S = []\n",
    "for m,t in M_T:\n",
    "    M_T_S.append([m,t])\n",
    "M_T_S[0].append(\"allegro/herbert\")\n",
    "M_T_S[1].append(\"dkleczek/bert\")\n",
    "M_T_S[2].append(\"henryk/bert\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: allegro/herbert\n",
      "\tString : Warszawa to największe <mask>\n",
      "\t\t0 : miasto\n",
      "\t\t1 : …\n",
      "\t\t2 : miasta\n",
      "\t\t3 : lotnisko\n",
      "\t\t4 : centrum\n",
      "\tString : Te zabawki należą do <mask>\n",
      "\t\t0 : …\n",
      "\t\t1 : rodziny\n",
      "\t\t2 : najlepszych\n",
      "\t\t3 : :\n",
      "\t\t4 : nich\n",
      "\tString : Policjant przygląda się <mask>\n",
      "\t\t0 : policji\n",
      "\t\t1 : …\n",
      "\t\t2 : kobiecie\n",
      "\t\t3 : mężczyźnie\n",
      "\t\t4 : policjantom\n",
      "\tString : Na środku skrzyżowania widać <mask>\n",
      "\t\t0 : …\n",
      "\t\t1 : :\n",
      "\t\t2 : .\n",
      "\t\t3 : samochód\n",
      "\t\t4 : skrzyżowanie\n",
      "\tString : Właściciel samochodu widział złodzieja z <mask>\n",
      "\t\t0 : włamaniem\n",
      "\t\t1 : samochodu\n",
      "\t\t2 : parkingu\n",
      "\t\t3 : bronią\n",
      "\t\t4 : policji\n",
      "\tString : Prezydent z premierem rozmawiali wczoraj o <mask>\n",
      "\t\t0 : …\n",
      "\t\t1 : polityce\n",
      "\t\t2 : bezpieczeństwie\n",
      "\t\t3 : Polsce\n",
      "\t\t4 : projekcie\n",
      "\tString : Witaj drogi <mask>\n",
      "\t\t0 : !\n",
      "\t\t1 : .\n",
      "\t\t2 : ,\n",
      "\t\t3 : Boże\n",
      "\t\t4 : …\n",
      "Name: dkleczek/bert\n",
      "\tString : Warszawa to największe [MASK]\n",
      "\t\t0 : miasto\n",
      "\t\t1 : .\n",
      "\t\t2 : miejsce\n",
      "\t\t3 : miasta\n",
      "\t\t4 : pole\n",
      "\tString : Te zabawki należą do [MASK]\n",
      "\t\t0 : mnie\n",
      "\t\t1 : ciebie\n",
      "\t\t2 : niego\n",
      "\t\t3 : pana\n",
      "\t\t4 : nas\n",
      "\tString : Policjant przygląda się [MASK]\n",
      "\t\t0 : .\n",
      "\t\t1 : :\n",
      "\t\t2 : !\n",
      "\t\t3 : ,\n",
      "\t\t4 : mu\n",
      "\tString : Na środku skrzyżowania widać [MASK]\n",
      "\t\t0 : :\n",
      "\t\t1 : .\n",
      "\t\t2 : ,\n",
      "\t\t3 : napis\n",
      "\t\t4 : ulice\n",
      "\tString : Właściciel samochodu widział złodzieja z [MASK]\n",
      "\t\t0 : tyłu\n",
      "\t\t1 : samochodu\n",
      "\t\t2 : bliska\n",
      "\t\t3 : parkingu\n",
      "\t\t4 : ulicy\n",
      "\tString : Prezydent z premierem rozmawiali wczoraj o [MASK]\n",
      "\t\t0 : tym\n",
      "\t\t1 : panu\n",
      "\t\t2 : tobie\n",
      "\t\t3 : spotkaniu\n",
      "\t\t4 : zamachu\n",
      "\tString : Witaj drogi [MASK]\n",
      "\t\t0 : .\n",
      "\t\t1 : !\n",
      "\t\t2 : chłopcze\n",
      "\t\t3 : przyjacielu\n",
      "\t\t4 : bracie\n",
      "Name: henryk/bert\n",
      "\tString : Warszawa to największe [MASK]\n",
      "\t\t0 : 漣\n",
      "\t\t1 : Srpska\n",
      "\t\t2 : 濁\n",
      "\t\t3 : független\n",
      "\t\t4 : Kraj\n",
      "\tString : Te zabawki należą do [MASK]\n",
      "\t\t0 : номи\n",
      "\t\t1 : enw\n",
      "\t\t2 : скончался\n",
      "\t\t3 : ##ンジ\n",
      "\t\t4 : йочанаш\n",
      "\tString : Policjant przygląda się [MASK]\n",
      "\t\t0 : Generalitat\n",
      "\t\t1 : ##ンジ\n",
      "\t\t2 : იმ\n",
      "\t\t3 : ##ковић\n",
      "\t\t4 : 濁\n",
      "\tString : Na środku skrzyżowania widać [MASK]\n",
      "\t\t0 : Suva\n",
      "\t\t1 : konce\n",
      "\t\t2 : ##agt\n",
      "\t\t3 : თბ\n",
      "\t\t4 : იმ\n",
      "\tString : Właściciel samochodu widział złodzieja z [MASK]\n",
      "\t\t0 : ##ever\n",
      "\t\t1 : ##odet\n",
      "\t\t2 : ##eall\n",
      "\t\t3 : ##şık\n",
      "\t\t4 : 漣\n",
      "\tString : Prezydent z premierem rozmawiali wczoraj o [MASK]\n",
      "\t\t0 : exist\n",
      "\t\t1 : ##eall\n",
      "\t\t2 : ##odet\n",
      "\t\t3 : ##ンジ\n",
      "\t\t4 : 漣\n",
      "\tString : Witaj drogi [MASK]\n",
      "\t\t0 : UCI\n",
      "\t\t1 : Farnese\n",
      "\t\t2 : генерального\n",
      "\t\t3 : Generalitat\n",
      "\t\t4 : ##нског\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "for model,tokenizer,name in M_T_S:\n",
    "    print(\"Name: {}\".format(name))\n",
    "    \n",
    "    s1 = f\"Warszawa to największe {tokenizer.mask_token}\"\n",
    "    s2 = f\"Te zabawki należą do {tokenizer.mask_token}\"\n",
    "    s3 = f\"Policjant przygląda się {tokenizer.mask_token}\"\n",
    "    s4 = f\"Na środku skrzyżowania widać {tokenizer.mask_token}\"\n",
    "    s5 = f\"Właściciel samochodu widział złodzieja z {tokenizer.mask_token}\"\n",
    "    s6 = f\"Prezydent z premierem rozmawiali wczoraj o {tokenizer.mask_token}\"\n",
    "    s7 = f\"Witaj drogi {tokenizer.mask_token}\"\n",
    "    strs = (s1,s2,s3,s4,s5,s6,s7)\n",
    "\n",
    "    for sequence in strs:\n",
    "        print(\"\\tString : {}\".format(sequence))\n",
    "\n",
    "        input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "        mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "\n",
    "        token_logits = model(input).logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "        i = 0\n",
    "        for token in top_5_tokens:\n",
    "            print(\"\\t\\t{} : {}\".format(i,tokenizer.decode([token])))\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: allegro/herbert\n",
      "\tString : Gdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie <mask>\n",
      "\t\t0 : dowiedział\n",
      "\t\t1 : zdziwił\n",
      "\t\t2 : poddał\n",
      "\t\t3 : zastanawiał\n",
      "\t\t4 : zorientował\n",
      "\tString : Gdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie <mask>\n",
      "\t\t0 : dowiedziała\n",
      "\t\t1 : bała\n",
      "\t\t2 : przyznała\n",
      "\t\t3 : zmieniła\n",
      "\t\t4 : śmiała\n",
      "Name: dkleczek/bert\n",
      "\tString : Gdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie [MASK]\n",
      "\t\t0 : dowiedział\n",
      "\t\t1 : martwił\n",
      "\t\t2 : przejmował\n",
      "\t\t3 : zgodził\n",
      "\t\t4 : przyznał\n",
      "\tString : Gdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie [MASK]\n",
      "\t\t0 : dowiedziała\n",
      "\t\t1 : zgodziła\n",
      "\t\t2 : poddała\n",
      "\t\t3 : martwiła\n",
      "\t\t4 : zabiła\n",
      "Name: henryk/bert\n",
      "\tString : Gdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie [MASK]\n",
      "\t\t0 : Pardosa\n",
      "\t\t1 : dele\n",
      "\t\t2 : தேர்வுத்\n",
      "\t\t3 : liner\n",
      "\t\t4 : диплом\n",
      "\tString : Gdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie [MASK]\n",
      "\t\t0 : Pardosa\n",
      "\t\t1 : liner\n",
      "\t\t2 : 漣\n",
      "\t\t3 : தேர்வுத்\n",
      "\t\t4 : диплом\n"
     ]
    }
   ],
   "source": [
    "for model,tokenizer,name in M_T_S:\n",
    "    print(\"Name: {}\".format(name))\n",
    "    s1 = f\"Gdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie {tokenizer.mask_token}\"\n",
    "    s2 = f\"Gdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie {tokenizer.mask_token}\"\n",
    "    \n",
    "    strs = (s1,s2)\n",
    "    for sequence in strs:\n",
    "        print(\"\\tString : {}\".format(sequence))\n",
    "\n",
    "        input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "        mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "\n",
    "        token_logits = model(input).logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "        i = 0\n",
    "        for token in top_5_tokens:\n",
    "            print(\"\\t\\t{} : {}\".format(i,tokenizer.decode([token])))\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: allegro/herbert\n",
      "\tString : <mask> wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t0 : Woda\n",
      "\t\t1 : Słońce\n",
      "\t\t2 : Ziemia\n",
      "\t\t3 : Następnie\n",
      "\t\t4 : Ciało\n",
      "\tString : W wakacje odwiedziłem <mask>, który jest stolicą Islandii\n",
      "\t\t0 : Oslo\n",
      "\t\t1 : Kraków\n",
      "\t\t2 : Londyn\n",
      "\t\t3 : Paryż\n",
      "\t\t4 : Gdańsk\n",
      "\tString : Informatyka na <mask> należy do najlepszych kierunków w Polsce.\n",
      "\t\t0 : pewno\n",
      "\t\t1 : AGH\n",
      "\t\t2 : UW\n",
      "\t\t3 : studiach\n",
      "\t\t4 : UMK\n",
      "Name: dkleczek/bert\n",
      "\tString : [MASK] wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t0 : woda\n",
      "\t\t1 : ciało\n",
      "\t\t2 : nie\n",
      "\t\t3 : lod\n",
      "\t\t4 : powietrze\n",
      "\tString : W wakacje odwiedziłem [MASK], który jest stolicą Islandii\n",
      "\t\t0 : kraj\n",
      "\t\t1 : zamek\n",
      "\t\t2 : swiat\n",
      "\t\t3 : hotel\n",
      "\t\t4 : park\n",
      "\tString : Informatyka na [MASK] należy do najlepszych kierunków w Polsce.\n",
      "\t\t0 : uniwersytecie\n",
      "\t\t1 : swiecie\n",
      "\t\t2 : uczelni\n",
      "\t\t3 : uczelniach\n",
      "\t\t4 : ukrainie\n",
      "Name: henryk/bert\n",
      "\tString : [MASK] wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\n",
      "\t\t0 : Generalitat\n",
      "\t\t1 : 漣\n",
      "\t\t2 : йовхачу\n",
      "\t\t3 : 濁\n",
      "\t\t4 : ##мку\n",
      "\tString : W wakacje odwiedziłem [MASK], który jest stolicą Islandii\n",
      "\t\t0 : եկել\n",
      "\t\t1 : месец\n",
      "\t\t2 : compose\n",
      "\t\t3 : unclear\n",
      "\t\t4 : spelt\n",
      "\tString : Informatyka na [MASK] należy do najlepszych kierunków w Polsce.\n",
      "\t\t0 : konce\n",
      "\t\t1 : ##ンジ\n",
      "\t\t2 : ##nist\n",
      "\t\t3 : 漣\n",
      "\t\t4 : Федератсияи\n"
     ]
    }
   ],
   "source": [
    "for model,tokenizer,name in M_T_S:\n",
    "    print(\"Name: {}\".format(name))\n",
    "    s1 = f\"{tokenizer.mask_token} wrze w temperaturze 100 stopni, a zamarza w temeraturze 0 stopni Celsjusza.\"\n",
    "    s2 = f\"W wakacje odwiedziłem {tokenizer.mask_token}, który jest stolicą Islandii\"\n",
    "    s3 = f\"Informatyka na {tokenizer.mask_token} należy do najlepszych kierunków w Polsce.\"\n",
    "    \n",
    "    strs = (s1,s2,s3)\n",
    "    for sequence in strs:\n",
    "        print(\"\\tString : {}\".format(sequence))\n",
    "\n",
    "        input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "        mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "\n",
    "        token_logits = model(input).logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "        i = 0\n",
    "        for token in top_5_tokens:\n",
    "            print(\"\\t\\t{} : {}\".format(i,tokenizer.decode([token])))\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer the following questions:\n",
    "\n",
    "#### Which of the models produced the best results?\n",
    "\n",
    "The best results were obtained from allegro/herbert-base-cased. It has knowledge in the model, it is sensitive to relations between long distanced words and takes into account polish grammar as well.\n",
    "\n",
    "#### Was any of the models able to capture Polish grammar?\n",
    "Yes allegro/herbert-base-cased could do it. For example Warszawa to największe MIASTO or Te zabawki należą do RODZINY.\n",
    "\n",
    "Also dkleczek/bert was quite successful with Te zabawki należą do RODZINY or Warszawa to największe MIASTO.\n",
    "\n",
    "\n",
    "#### Was any of the models able to capture long-distant relationships between the words?\n",
    "\n",
    "Yes both allegro/herbert-base-cased and dkleczek/bert were passed that test\n",
    "\n",
    "\n",
    "#### Was any of the models able to capture world knowledge?\n",
    "\n",
    "We can see that allegro/herbert-base-cased captured knowledge pretty well with boiling water and studies in Poland. It was not so successful when it comes to the capital of Iceland but it lists some capitals of other countries so we can assume that it was a good shot.\n",
    "\n",
    "Dkleczek/bert was not so good as predecessor. It was only successful when it comes to boiling. \n",
    "\n",
    "\n",
    "#### What are the most striking errors made by the models?\n",
    "\n",
    "We can say that the most striking error was choosing henryk/bert-base-multilingual-cased-finetuned-polish-squad1 because it produced no good result.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
